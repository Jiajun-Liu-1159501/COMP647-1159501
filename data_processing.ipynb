{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a871d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data manipulation and analysis.\n",
    "import numpy as np # Numerical operations and array handling.\n",
    "import matplotlib.pyplot as plt # More control, lower-level, basic plotting.\n",
    "import seaborn as sns # Higher-level, more aesthetically pleasing plots.\n",
    "from scipy import stats # Statistical functions and tests.\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Display all columns in DataFrame output.\n",
    "pd.set_option('display.max_rows', None) # Display all rows in DataFrame output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc42e51",
   "metadata": {},
   "source": [
    "### Load all CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad766de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/xiaowenrou/Downloads/COMP647/financial_fraud_detection_dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db5b76",
   "metadata": {},
   "source": [
    "### Remove irrelevant data\n",
    "\n",
    "When building a fraud detection model, each feature must be available in real-time in real-world transaction processing scenarios and have the ability to distinguish between legitimate and fraudulent transactions.\n",
    "\n",
    "- `transaction_id`: The transaction ID is a unique identifier generated by the system and does not contain any predictive information.\n",
    "- `sender_account` & `receiver_account`: Account identifiers can lead to model overfitting: most transactions involve new account combinations that have never been seen before.\n",
    "- `fraud_type`: We only need binary classification, not classification of fraud types, so remove this column.\n",
    "- `ip_address`: IP addresses are dynamically allocated, so the same user may use different IP addresses, and different users may share the same IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    'transaction_id',\n",
    "    'sender_account',\n",
    "    'receiver_account',\n",
    "    'fraud_type',\n",
    "    'ip_address'\n",
    "]\n",
    "\n",
    "df = df.drop(columns = columns_to_remove)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79541a44",
   "metadata": {},
   "source": [
    "### Remove Null value rows\n",
    "In this section, we use different data checking strategies for different data types.\n",
    "\n",
    "- For categorical variables (`transaction_type`, `merchant_category`, `location`, `device_used`, `payment_channel`, `device_hash`), a complete deletion strategy is adopted. When missing values ​​appear in these fields, the entire row of data containing the missing value is directly deleted. For the missing of such categorical variables, forced filling will introduce noise and may mislead model learning.\n",
    "\n",
    "- For numeric variables (`amount`, `time_since_last_transaction`, `spending_deviation_score`, `velocity_score`, `geo_anomaly_score`), the fill method is used. `time_since_last_transaction`: Missing values ​​are filled with -1 to specifically identify first-time transaction users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "rows_before = len(df)\n",
    "\n",
    "# Handling categorical variables: remove the row if missing\n",
    "categorical_cols = ['transaction_type', 'merchant_category', 'location', 'device_used', 'payment_channel', 'device_hash']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns and df[col].isnull().any():\n",
    "        missing_count_before = df[col].isnull().sum()\n",
    "        df = df.dropna(subset=[col])\n",
    "\n",
    "# Handling numbric variables: \n",
    "numerical_cols = ['amount', 'time_since_last_transaction', 'spending_deviation_score', 'velocity_score', 'geo_anomaly_score']\n",
    "for col in numerical_cols:\n",
    "    if col in df.columns and df[col].isnull().any():\n",
    "        if col == 'time_since_last_transaction':\n",
    "            # For first-time transaction users, setting this to -1 means no transaction history\n",
    "            df[col] = df[col].fillna(-1)\n",
    "        else:\n",
    "            # Other values ​​are filled with median\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "rows_after = len(df)\n",
    "print(f\"{rows_before - rows_after} rows are removed from data set\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d3c58",
   "metadata": {},
   "source": [
    "### Remove Duplicates and Constant Features\n",
    "\n",
    "- Duplicate value handling strategy: Use a complete deletion strategy, retaining the first record in each set of duplicate data. This approach is simple and direct, avoiding errors that may be introduced by complex duplicate data merging logic.\n",
    "- Constant feature identification strategy: Each feature column is checked for the number of unique values ​​`nunique()`. If the number of unique values ​​is less than or equal to 1, the feature is considered a constant feature. All constant feature columns are deleted, but the target variable `is_fraud` is particularly protected. Even in extremely imbalanced datasets, the target variable should not be mistakenly deleted because it is the core basis for model learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_values = df.duplicated().sum()\n",
    "print(f\"Duplicated rows: {duplicated_values}\")\n",
    "\n",
    "# Remove duplicate rows if found\n",
    "if duplicated_values > 0:\n",
    "   df = df.drop_duplicates()\n",
    "   print(f\"{duplicated_values} duplicated rows are removed from data set\")\n",
    "\n",
    "# Remove Constant Features  \n",
    "constant_features = []\n",
    "for col in df.columns:\n",
    "   if col != 'is_fraud':\n",
    "       if df[col].nunique() <= 1:\n",
    "           constant_features.append(col)\n",
    "\n",
    "if constant_features:\n",
    "   df = df.drop(columns=constant_features)\n",
    "   print(f\"{len(constant_features)} constant features are removed: {constant_features}\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a57d40",
   "metadata": {},
   "source": [
    "### Remove outliers rows\n",
    "Fraudulent transactions are often \"abnormal.\" \"Outliers\" such as unusually high transaction amounts, unusual geographic locations, and unusual transaction frequencies can be important features for identifying fraud.\n",
    "Therefore, the outlier handling strategy is to examine the outlier distributions of normal and fraudulent samples separately.\n",
    "\n",
    "- `amount`: Identifies unusually large or small transactions\n",
    "- `time_since_last_transaction`: Detects unusually frequent or rare transaction patterns\n",
    "- `spending_deviation_score`: Captures anomalies that significantly deviate from a user's historical behavior\n",
    "- `velocity_score`: Identifies unusual transaction velocity patterns\n",
    "- `geo_anomaly_score`: Detects geographic anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d50007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define numerical columns for outlier detection\n",
    "outlier_cols = ['amount', 'time_since_last_transaction', 'spending_deviation_score', 'velocity_score', 'geo_anomaly_score']\n",
    "\n",
    "df_before_outlier_removal = df.copy()\n",
    "\n",
    "fraud_data = df[df['is_fraud'] == True]\n",
    "non_fraud_data = df[df['is_fraud'] == False]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, len(outlier_cols), figsize=(20, 4))\n",
    "for i, col in enumerate(outlier_cols):\n",
    "    if col in df.columns:\n",
    "        data = df[col].dropna()\n",
    "        stats.probplot(data, dist=\"norm\", plot=axes[i])\n",
    "        axes[i].set_title(f'Total Samples - {col}\\nProbability Plot')\n",
    "        axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, len(outlier_cols), figsize=(20, 4))\n",
    "for i, col in enumerate(outlier_cols):\n",
    "    if col in df.columns:\n",
    "        data = fraud_data[col].dropna()\n",
    "        stats.probplot(data, dist=\"norm\", plot=axes[i])\n",
    "        axes[i].set_title(f'Fraud Samples - {col}\\nProbability Plot')\n",
    "        axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(outlier_cols), figsize=(20, 4))\n",
    "for i, col in enumerate(outlier_cols):\n",
    "    if col in df.columns:\n",
    "        data = non_fraud_data[col].dropna()\n",
    "        stats.probplot(data, dist=\"norm\", plot=axes[i])\n",
    "        axes[i].set_title(f'Non-Fraud Samples - {col}\\nProbability Plot')\n",
    "        axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c82456",
   "metadata": {},
   "source": [
    "A comparison reveals that the distribution of each feature is nearly identical in both normal and fraudulent samples.\n",
    "\n",
    "- `amount`: Distribution clearly right-skewed, with extreme values.\n",
    "\n",
    "- `time_since_last_transaction`: Distribution clearly right-skewed, with a significant clustering of extreme values.\n",
    "\n",
    "- `spending_deviation_score`: Closest to a normal distribution, but with slight deviations at either end.\n",
    "\n",
    "- `velocity_score`: Distribution clearly right-skewed, with a long tail.\n",
    "\n",
    "- `Geo_anomaly_score`: Distribution relatively mildly right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae25191",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['transaction_type', 'merchant_category', 'location', 'device_used', 'payment_channel']\n",
    "\n",
    "for col in categorical_features:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    cross_tab = pd.crosstab(df[col], df['is_fraud'])\n",
    "    \n",
    "    cross_tab.plot(kind='bar', stacked=True, ax=plt.gca(), color=['lightblue', 'red'], alpha=0.7)\n",
    "    \n",
    "    plt.title(f'{col} Distribution by Fraud Status')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Non-Fraud', 'Fraud'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8f9f1",
   "metadata": {},
   "source": [
    "In fraud detection projects, analyzing the distribution of classification features by fraud status is a key step in the process. Fraudsters often have specific behavior patterns and preferences: they may prefer certain transaction types (such as transfers, withdrawals, or use specific device types or payment channels to evade detection).\n",
    "However, statistics reveal that the number of each group tends to be average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307ebcd",
   "metadata": {},
   "source": [
    "### Calculate fraud transaction rate\n",
    "In fraud detection scenarios, normal transactions often far outnumber fraudulent transactions, resulting in a highly unbalanced data distribution. Fraud rate calculation can accurately quantify the degree of this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rate = df['is_fraud'].mean()\n",
    "normal_rate = 1 - fraud_rate\n",
    "\n",
    "categories = ['Non-Fraud', 'Fraud']\n",
    "rates = [normal_rate, fraud_rate]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(categories, rates, color=['skyblue', 'red'], alpha=0.7)\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Sample Distribution')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for bar, rate in zip(bars, rates):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{rate:.2%}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483f2ec",
   "metadata": {},
   "source": [
    "Feature Creation\n",
    "\n",
    "This step converts the original timestamp data into categorical features with clear business meaning. In financial fraud detection, the time dimension contains rich behavioral pattern information. Normal user transaction behavior usually follows predictable time patterns.\n",
    "\n",
    "- `is_business_hours`: This period is usually the main window for normal business activities.\n",
    "- `is_late_night`: Identify late-night trading between 10 PM and 6 AM, as this period of time is considered unusual.\n",
    "- `is_weekend`: Distinguishing between weekday and weekend trading patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)\n",
    "df['is_late_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Remove helper columns\n",
    "columns_to_remove = [\n",
    "    'timestamp',\n",
    "    'day_of_week',\n",
    "    'hour',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_remove)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
