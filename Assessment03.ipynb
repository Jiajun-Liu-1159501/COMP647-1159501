{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e7ca99",
   "metadata": {},
   "source": [
    "## Device initialization\n",
    "\n",
    "This section initializes the computational environment for the project. It performs three critical tasks:\n",
    "\n",
    "1. Detects available hardware (CPU vs GPU)\n",
    "2. Configures PyTorch to use the optimal compute device\n",
    "3. Verifies the setup by displaying device information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fc9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available? False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Data manipulation and analysis.\n",
    "import numpy as np # Numerical operations and array handling.\n",
    "import matplotlib.pyplot as plt # More control, lower-level, basic plotting.\n",
    "import seaborn as sns # Higher-level, more aesthetically pleasing plots.\n",
    "from scipy import stats # Statistical functions and tests.\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Display all columns in DataFrame output.\n",
    "pd.set_option('display.max_rows', None) # Display all rows in DataFrame output.\n",
    "\n",
    "import torch\n",
    "import types\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "import copy\n",
    "\n",
    "# Check if CUDA (GPU) is available for acceleration\n",
    "print('is cuda available?', torch.cuda.is_available())\n",
    "\n",
    "# If CUDA is available, print the name of the GPU device\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2e7ce",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "This section loads the SST dataset from local files and processes it for binary sentiment classification.\n",
    "\n",
    "4 files are loaded from the `SST2-Data/stanfordSentimentTreebank/` folder:\n",
    "\n",
    "### 1. `datasetSentences.txt`\n",
    "\n",
    "Contains all sentences extracted from movie reviews, this is the primary data source containing the text we want to classify.\n",
    "\n",
    "- Column 1: `sentence_index` - Unique identifier for each sentence\n",
    "- Column 2: `sentence` - The actual text of the sentence\n",
    "\n",
    "### 2. `datasetSplit.txt`\n",
    "\n",
    "Specifies which dataset split each sentence belongs to, ensures using the standard train/dev/test splits.\n",
    "\n",
    "- Column 1: `sentence_index` - Links to sentences in `datasetSentences.txt`\n",
    "- Column 2: `splitset_label` - Split assignment\n",
    "  - **1** = Training set\n",
    "  - **2** = Test set\n",
    "  - **3** = Development/Validation set\n",
    "\n",
    "### 3. `dictionary.txt`\n",
    "\n",
    "Maps all phrases (including sub-phrases) to unique IDs, acts as a lookup table to connect sentences to their sentiment labels, as a bridge between `datasetSentences.txt` and `sentiment_labels.txt`.\n",
    "\n",
    "- Column 1: `phrase` - Text of the phrase (can be a word, sub-phrase, or complete sentence)\n",
    "- Column 2: `phrase_id` - Unique integer identifier\n",
    "\n",
    "### 4. `sentiment_labels.txt`\n",
    "\n",
    "Contains sentiment scores for all phrases, provides the ground truth labels for training our sentiment classifier.\n",
    "\n",
    "- Column 1: `phrase ids` - Links to `phrase_id` in `dictionary.txt`\n",
    "- Column 2: `sentiment values` - Continuous sentiment score from 0 (most negative) to 1 (most positive)\n",
    "\n",
    "There are 5 sentiments (very negative, negative, neutral, positive, very positive) in original dataset, but this project we only need binary classification (negative or positive), thus the sentences with sentiment values between 0.4 and 0.6 should be removed. Then convert labels to integers (0 or 1)\n",
    "\n",
    "```\n",
    "0.0 ←−−−−−−− 0.2 ←−−−− 0.4 ←− 0.5 −→ 0.6 −−−−→ 0.8 −−−−−−→ 1.0\n",
    "Very Negative | Negative | Neutral | Positive | Very Positive\n",
    "```\n",
    "---\n",
    "\n",
    "###  Final Format\n",
    "\n",
    "After processing, each DataFrame has this structure:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `sentence` | string | Movie review text |\n",
    "| `label` | int | Binary sentiment |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceefff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 11855\n",
      "Dictionary phrases: 239232\n",
      "Labels: 239232\n",
      "\n",
      "After filtering neutral samples: 9142\n",
      "Label distribution:\n",
      "label\n",
      "1    4739\n",
      "0    4403\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: 6568, Val: 825, Test: 1749\n"
     ]
    }
   ],
   "source": [
    "# Load the four essential files from Stanford Sentiment Treebank dataset\n",
    "# These files are interconnected and need to be merged to create our final dataset\n",
    "\n",
    "# Load all sentences with their unique indices\n",
    "sentences_df = pd.read_csv('SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/datasetSentences.txt', sep='\\t', header=0)\n",
    "\n",
    "# Load dataset split assignments (train/dev/test)\n",
    "split_df = pd.read_csv('SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/datasetSplit.txt', sep=',', header=0)\n",
    "\n",
    "# Load phrase dictionary (maps phrases to unique IDs)\n",
    "dictionary_df = pd.read_csv('SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/dictionary.txt', sep='|', header=None, names=['phrase', 'phrase_id'])\n",
    "\n",
    "# Load sentiment labels for all phrases\n",
    "labels_df = pd.read_csv('SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt', sep='|', header=0)\n",
    "\n",
    "print(f\"Sentences: {len(sentences_df)}\")           # Expected: 11,855\n",
    "print(f\"Dictionary phrases: {len(dictionary_df)}\") # Expected: 239,232\n",
    "print(f\"Labels: {len(labels_df)}\")                 # Expected: 239,232\n",
    "\n",
    "# Combine sentences with their train/dev/test assignments\n",
    "# This creates a unified DataFrame with: sentence_index, sentence, splitset_label\n",
    "data = sentences_df.merge(split_df, on='sentence_index')\n",
    "\n",
    "# Match Sentences to Sentiment Labels\n",
    "# Process: Sentence Text → Dictionary (get phrase_id) → Labels (get sentiment) → Binary Label\n",
    "\n",
    "# Initialize empty dictionary to store sentence-to-label mappings\n",
    "sentence_labels = {}\n",
    "\n",
    "# Iterate through each sentence in the merged dataset\n",
    "for idx, row in data.iterrows():\n",
    "    sentence = row['sentence']  # Extract the sentence text\n",
    "    \n",
    "    # Look up phrase_id in dictionary\n",
    "    # Find the dictionary entry where phrase matches the current sentence\n",
    "    phrase_match = dictionary_df[dictionary_df['phrase'] == sentence]\n",
    "    \n",
    "    # Check if we found a match (some sentences might not be in dictionary)\n",
    "    if not phrase_match.empty:\n",
    "        # Extract the phrase_id for this sentence\n",
    "        phrase_id = phrase_match.iloc[0]['phrase_id']\n",
    "        \n",
    "        # Look up sentiment score using phrase_id\n",
    "        # Find the label entry where phrase_id matches\n",
    "        label_match = labels_df[labels_df['phrase ids'] == phrase_id]\n",
    "        \n",
    "        # Check if we found a sentiment score\n",
    "        if not label_match.empty:\n",
    "            # Extract the continuous sentiment value (0.0 to 1.0)\n",
    "            sentiment_value = label_match.iloc[0]['sentiment values']\n",
    "            \n",
    "            # Convert continuous sentiment to binary label\n",
    "            # Apply threshold-based conversion for binary classification\n",
    "            \n",
    "            if sentiment_value <= 0.4:\n",
    "                # Negative class: very negative + negative samples\n",
    "                # Range: [0.0, 0.4]\n",
    "                sentence_labels[sentence] = 0\n",
    "                \n",
    "            elif sentiment_value >= 0.6:\n",
    "                # Positive class: positive + very positive samples\n",
    "                # Range: [0.6, 1.0]\n",
    "                sentence_labels[sentence] = 1\n",
    "\n",
    "# Apply Labels and Filter Neutral Samples\n",
    "# Map the sentence_labels dictionary to create a new 'label' column\n",
    "# Sentences not in sentence_labels (neutral samples) will have NaN values\n",
    "data['label'] = data['sentence'].map(sentence_labels)\n",
    "\n",
    "# Remove all rows where label is NaN\n",
    "data = data.dropna(subset=['label'])\n",
    "\n",
    "# Convert label column from float to integer type.\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Display statistics after filtering\n",
    "print(f\"\\nAfter filtering neutral samples: {len(data)}\")  # Expected: ~9,613\n",
    "print(f\"Label distribution:\\n{data['label'].value_counts()}\")  # Should be ~50/50 split\n",
    "\n",
    "# Split Data into Train/Validation/Test Sets\n",
    "# Use the splitset_label column to separate data according to standard splits\n",
    "\n",
    "# Training set (splitset_label == 1), keep only 'sentence' and 'label' columns, drop unnecessary columns\n",
    "# reset_index(drop=True) renumbers rows from 0 to len-1 for clean indexing\n",
    "train_df = data[data['splitset_label'] == 1][['sentence', 'label']].reset_index(drop=True)\n",
    "\n",
    "# Test set (splitset_label == 2)\n",
    "test_df = data[data['splitset_label'] == 2][['sentence', 'label']].reset_index(drop=True)\n",
    "\n",
    "# Validation/Development set (splitset_label == 3)\n",
    "val_df = data[data['splitset_label'] == 3][['sentence', 'label']].reset_index(drop=True)\n",
    "\n",
    "# Display final dataset sizes\n",
    "print(f\"\\nTrain: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730c14a",
   "metadata": {},
   "source": [
    "## Dataset processing\n",
    "\n",
    "This section transforms raw text data into numerical representations that neural networks can process. Text strings cannot be directly fed into neural networks - they must first be converted into sequences of integers.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Split sentences into individual words and punctuation marks. Neural networks operate on discrete units. We also keep punctuation as separate tokens because they carry sentiment information\n",
    "\n",
    "### Vocabulary Construction\n",
    "\n",
    "Build a mapping between words and unique integer IDs (word → ID), a consistent way to convert any word into a number is required, including:\n",
    "\n",
    "- Filter rare words (frequency < 2): Reduces vocabulary size and filters potential typos/noise\n",
    "- Add special tokens: \n",
    "  - `<pad>`: For making sequences equal length (required for batch processing)\n",
    "  - `<unk>`: For handling words not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccf7c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6864\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: Breaking down text into individual words and punctuation marks\n",
    "# This is the first step in converting raw text into numerical representations\n",
    "\n",
    "_tok = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def tokenize(s: str):\n",
    "    \"\"\"\n",
    "        Convert a sentence string into a list of lowercase tokens.\n",
    "        \"\"\"\n",
    "    return [t.lower() for t in _tok.findall(s)]\n",
    "\n",
    "# Build Vocabulary from Training Data\n",
    "# This allows to convert text (strings) into numbers that neural networks can process\n",
    "# Define special tokens\n",
    "UNK = \"<unk>\"  # Unknown token: used for words not in vocabulary (out-of-vocabulary words)\n",
    "PAD = \"<pad>\"  # Padding token: used to make all sequences the same length\n",
    "\n",
    "# Initialize a Counter to count word frequencies\n",
    "# Counter is like a dictionary that automatically counts occurrences\n",
    "counter = Counter()\n",
    "\n",
    "# Count all words in the training set, only use training data to build vocabulary to prevent data leakage\n",
    "for sentence in train_df['sentence']:\n",
    "    # Tokenize each sentence and update word counts\n",
    "    counter.update(tokenize(sentence))\n",
    "# Build the vocabulary list (index-to-string mapping)\n",
    "itos = [PAD, UNK] + [w for w, c in counter.most_common() if c >= 2]\n",
    " # Build reverse mapping: string-to-index dictionary, allows quick lookup of a word's ID\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "# Get the padding token's index, will be used later to tell the model to ignore padding positions\n",
    "padding_idx = stoi[PAD]  # padding_idx = 0\n",
    "# Calculate vocabulary size\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e9258",
   "metadata": {},
   "source": [
    "## Loading Pre-trained Word Embeddings\n",
    "\n",
    "This code loads pre-trained GloVe embeddings and integrates them with our vocabulary. Instead of learning word representations from scratch, we leverage embeddings trained on massive text corpora. Pre-trained embeddings significantly improve model performance, especially when training data is limited.\n",
    "\n",
    "We now have a `TEXT` object containing our vocabulary mappings (`itos`, `stoi`) and an embedding matrix where known words start with meaningful representations, giving our model a significant advantage before training even begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b8ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Import the gensim downloader API for accessing pre-trained word embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe (Global Vectors for Word Representation) embeddings\n",
    "glv = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Set the dimensionality of word embeddings to match GloVe's dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# Initialize a tensor to store embedding vectors for all vocabulary words\n",
    "pretrained_vectors = torch.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "# Iterate through each token in the vocabulary\n",
    "for i, tok in enumerate(itos):\n",
    "    # Check if the current token exists in the pre-trained GloVe vocabulary\n",
    "    if tok in glv:\n",
    "        # If found, replace the random vector with the pre-trained GloVe vector\n",
    "        pretrained_vectors[i] = torch.tensor(glv[tok])\n",
    "\n",
    "# Set the embedding vector for the padding token to zeros\n",
    "pretrained_vectors[padding_idx] = 0.0\n",
    "\n",
    "# Create a namespace object to mimic the torchtext Field structure\n",
    "TEXT = types.SimpleNamespace(\n",
    "    vocab=types.SimpleNamespace(\n",
    "        itos=itos,                      # int-to-string: list mapping indices to tokens\n",
    "        stoi=stoi,                      # string-to-int: dict mapping tokens to indices\n",
    "        vectors=pretrained_vectors      # The embedding matrix with pre-trained vectors\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111263b",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section implements a feature engineering pipeline to establish a baseline understanding of sentiment classification. First extract hand-crafted linguistic features and evaluate their predictive power. This approach helps us understand what signals are important for sentiment analysis.\n",
    "\n",
    "There are 15 hand-crafted features extracted grouped into 7 categories, each capturing different aspects of sentiment expression:\n",
    "\n",
    "1. Length Features (Text Complexity)\n",
    "\n",
    "```python\n",
    "- len_tokens: Number of word tokens\n",
    "- len_chars: Total character count  \n",
    "- avg_tok_len: Average token length\n",
    "```\n",
    "\n",
    "- Positive reviews might be longer (detailed praise) or negative reviews might be longer (detailed complaints)\n",
    "- Longer words may indicate more formal or complex language\n",
    "- Text length reflects how much the author cared to express\n",
    "\n",
    "2. Punctuation Features (Emotional Expression)\n",
    "\n",
    "People expressing strong sentiment use more emphatic punctuation.\n",
    "\n",
    "```python\n",
    "- count_exclaim: '!' count (excitement/emphasis)\n",
    "- count_question: '?' count (uncertainty/questioning)\n",
    "- count_period: '.' count (sentence structure)\n",
    "- count_comma: ',' count (clause complexity)\n",
    "- count_punct_total: Overall punctuation density\n",
    "```\n",
    "\n",
    "- Exclamation marks signal strong emotion (both positive and negative)\n",
    "- Question marks often indicate confusion, doubt, or rhetorical emphasis\n",
    "- Commas suggest detailed, structured arguments (common in thoughtful reviews)\n",
    "- Punctuation density reflects writing style and emotional intensity\n",
    "\n",
    "3. Uppercase Features (Emphasis/Shouting)\n",
    "\n",
    "```python\n",
    "- count_upper_tokens: Number of ALL CAPS words\n",
    "- ratio_upper_tokens: Proportion of text in caps\n",
    "```\n",
    "\n",
    "- ALL CAPS = SHOUTING or EMPHASIS\n",
    "- Strong indicator of emotional intensity\n",
    "\n",
    "4. Elongation Features (Informal Emphasis)\n",
    "\n",
    "Character elongation mimics prosody (speech intonation) in written text.\n",
    "\n",
    "```python\n",
    "- count_elongated: Words with repeated characters\n",
    "```\n",
    "\n",
    "- Informal emphasis technique common in social media and reviews\n",
    "- Strong sentiment indicator: Positive: \"This is soooo good!\" Negative: \"Soooo disappointed\"\n",
    "\n",
    "---\n",
    "\n",
    "5. Negation Features (Sentiment Reversal)\n",
    "\n",
    "Simple bag-of-words models fail on negations - this feature explicitly captures them.\n",
    "\n",
    "```python\n",
    "- count_negations: Negation words (not, no, never, don't, can't, etc.)\n",
    "```\n",
    "\n",
    "- Negations reverse polarity: \"good\" → positive, \"not good\" → negative\n",
    "- Negations interact with surrounding words to flip meaning\n",
    "- High negation count may indicate negative sentiment (\"not good\", \"don't like\", \"never again\")\n",
    "\n",
    "6. Intensifier Features (Sentiment Amplification)\n",
    "\n",
    "Intensifiers modify adjectives to increase subjective intensity.\n",
    "\n",
    "```python\n",
    "- count_intensifiers: Intensifying adverbs (very, really, extremely, incredibly)\n",
    "```\n",
    "\n",
    "- Amplify sentiment strength without changing polarity\n",
    "- Indicate strong opinions and emotional engagement\n",
    "- Presence suggests the author felt compelled to emphasize their sentiment\n",
    "\n",
    "7. Emoticon Features (Direct Sentiment Signals)\n",
    "\n",
    "Emoticons serve as digital paralanguage, conveying emotion that would be expressed through facial expressions in person.\n",
    "\n",
    "```python\n",
    "- count_pos_emotes: Positive emoticons (:), :D, ^^)\n",
    "- count_neg_emotes: Negative emoticons (:(, :/, :'()\n",
    "```\n",
    "\n",
    "- Unambiguous sentiment indicators - direct emotional expression\n",
    "- Common in informal text (social media, casual reviews)\n",
    "- Often used to clarify tone: \"This is great :)\" vs. \"This is great :/\"\n",
    "\n",
    "### Feature Normalization Strategy\n",
    "\n",
    "All features are normalized to prevent scale imbalance:\n",
    "\n",
    "- Raw character count (100s) would overwhelm emoticon count (0-2)\n",
    "- A text with 100 tokens and 5 exclamations is different from 10 tokens and 5 exclamations\n",
    "\n",
    "using flowing methods:\n",
    "\n",
    "- Length features → Scaled by typical text size (20 tokens)\n",
    "- Punctuation → Divided by token or character count\n",
    "- Ratios → Already normalized (e.g., `ratio_upper_tokens`)\n",
    "- Emoticons → Raw counts (rare events, no scaling needed)\n",
    "\n",
    "### Chi-Square Feature Importance Test\n",
    "\n",
    "The Chi-Square test measures the statistical dependency between each feature and the sentiment label. A high χ² score indicates a strong association between the feature and sentiment, suggesting the feature is informative for prediction. Additionally, a low p-value confirms that this relationship is statistically significant and not due to random chance. \n",
    "\n",
    "We can keep features with high χ² scores (strong predictors) and discard those with low scores (weak or redundant predictors), ensuring our model focuses on the most informative signals while reducing noise and dimensionality.\n",
    "\n",
    "### Logistic Regression Baseline\n",
    "\n",
    "Logistic Regression serves as a linear model, it learns explicit weights for each feature, allowing us to understand exactly how much each linguistic pattern contributes to sentiment prediction. This transparency is valuable for validating our feature engineering decisions and building intuition about what matters for sentiment classification. \n",
    "\n",
    "An accuracy of 70-75% indicates that our hand-crafted features successfully capture significant sentiment signals, validating our feature engineering approach while leaving room for more sophisticated models to improve. If accuracy falls between 50-60%, it suggests our features are too weak and that we need representation learning techniques (like word embeddings and neural networks) to extract deeper semantic patterns. Conversely, if accuracy exceeds 80%, it may indicate that the classification problem is relatively simple, the dataset is small or homogeneous\n",
    "\n",
    "### Ablation Study\n",
    "\n",
    "Ablation analysis is a systematic experimental approach where we remove one feature group at a time and measure how much the model's performance degrades. By training separate models with each feature group excluded, we can directly observe each group's contribution to overall accuracy. A large accuracy drop indicates the removed feature group is critical and should be retained, a small or negligible drop suggests the group is redundant or already captured by other features, and paradoxically, if accuracy improves after removal, it means the feature group was adding noise rather than signal and should definitely be eliminated.\n",
    "\n",
    "Ablation analysis enables principled dimensionality reduction by revealing which features are truly unhelpful, allowing us to streamline our feature set and reduce noise without sacrificing predictive power. \n",
    "\n",
    "### Feature Selection Decision\n",
    "\n",
    "Use statistical significance (chi-square) as primary filter, validate with ablation study.\n",
    "\n",
    "1. Chi-square score > 10: Strong statistical association with sentiment\n",
    "2. Ablation study confirms importance**: Removing causes accuracy drop\n",
    "3. Interpretability: Feature makes linguistic sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fe1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feature validation dataset...\n",
      "Number of features: 15\n",
      "Training samples: 6568\n",
      "\n",
      "Feature Importance Ranking (Chi-square):\n",
      "                feature  chi2_score  p_value\n",
      "   count_negations_norm   11.013588 0.000904\n",
      "count_intensifiers_norm    5.073290 0.024297\n",
      "    count_question_norm    4.994573 0.025427\n",
      "     count_exclaim_norm    3.474562 0.062319\n",
      "         len_chars_norm    2.561395 0.109502\n",
      "       count_comma_norm    2.267597 0.132105\n",
      "      count_period_norm    1.148853 0.283789\n",
      "       avg_tok_len_norm    1.024558 0.311440\n",
      "count_upper_tokens_norm    1.018557 0.312862\n",
      "ratio_upper_tokens_norm    0.509278 0.475451\n",
      " count_punct_total_norm    0.328244 0.566695\n",
      "   count_elongated_norm    0.266407 0.605752\n",
      "        len_tokens_norm    0.000239 0.987673\n",
      "  count_pos_emotes_norm         NaN      NaN\n",
      "  count_neg_emotes_norm         NaN      NaN\n",
      "\n",
      "LR Baseline validation accuracy: 0.5952\n",
      "Conclusion: Hand-crafted features alone achieve 59.5% classification ability\n",
      "\n",
      "Ablation Study Results (performance after removing feature groups):\n",
      "removed_group  val_acc  acc_drop\n",
      "    negations 0.574545  0.020606\n",
      " intensifiers 0.589091  0.006061\n",
      "  punctuation 0.593939  0.001212\n",
      "    uppercase 0.595152  0.000000\n",
      "    emoticons 0.595152  0.000000\n",
      "    elongated 0.596364 -0.001212\n",
      "       length 0.598788 -0.003636\n",
      "\n",
      "============================================================\n",
      "PHASE 3 Summary: Feature Selection Decision\n",
      "============================================================\n",
      "Based on chi-square > 10.0, selected 1 features:\n",
      "   - count_negations_norm           (chi2=11.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score          # Metric to evaluate classification accuracy\n",
    "from sklearn.discriminant_analysis import StandardScaler  # Feature normalization (z-score scaling)\n",
    "from sklearn.feature_selection import chi2       # Chi-square test for feature importance\n",
    "from sklearn.linear_model import LogisticRegression  # Simple linear classifier as baseline\n",
    "\n",
    "# Words that negate or reverse sentiment (e.g., \"not good\" becomes negative)\n",
    "NEGATIONS = {\"not\",\"no\",\"never\",\"n't\",\"dont\",\"don't\",\"didn't\",\"won't\",\"cannot\",\"can't\"}\n",
    "\n",
    "# Words that amplify sentiment intensity (e.g., \"very good\" is stronger than \"good\")\n",
    "INTENSIFIERS = {\"very\",\"really\",\"so\",\"too\",\"extremely\",\"super\",\"highly\",\"utterly\",\"absolutely\",\"incredibly\"}\n",
    "\n",
    "# Emoticons indicating positive sentiment\n",
    "POS_EMOTES = [\":)\",\":-)\",\":d\",\"=)\",\":]\",\"^^\"]\n",
    "\n",
    "# Emoticons indicating negative sentiment\n",
    "NEG_EMOTES = [\":(\",\" :-(\",\" ):\",\":'(\",\" :/ \",\":-/\"]\n",
    "\n",
    "\n",
    "# These features capture linguistic patterns that correlate with sentiment\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"len_tokens\",           # Number of word tokens (vocabulary richness)\n",
    "    \"len_chars\",            # Total character count (text verbosity)\n",
    "    \"avg_tok_len\",          # Average token length (word complexity)\n",
    "    \n",
    "    \"count_exclaim\",        # '!' count (excitement/emphasis indicator)\n",
    "    \"count_question\",       # '?' count (questioning/uncertainty)\n",
    "    \"count_period\",         # '.' count (sentence structure)\n",
    "    \"count_comma\",          # ',' count (clause complexity)\n",
    "    \"count_punct_total\",    # Total punctuation (stylistic density)\n",
    "    \n",
    "    \"count_upper_tokens\",   # ALL CAPS words (shouting/emphasis)\n",
    "    \"ratio_upper_tokens\",   # Proportion of uppercase tokens\n",
    "    \n",
    "    \"count_elongated\",      # Words with repeated letters (e.g., \"soooo good\")\n",
    "    \"count_negations\",      # Negation words (sentiment reversal)\n",
    "    \"count_intensifiers\",   # Intensifier words (sentiment amplification)\n",
    "    \n",
    "    \"count_pos_emotes\",     # Positive emoticon count\n",
    "    \"count_neg_emotes\"      # Negative emoticon count\n",
    "]\n",
    "\n",
    "\n",
    "def is_all_caps_token(tok: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a token is written in ALL CAPS.\n",
    "    \"\"\"\n",
    "    # Extract only alphabetic characters\n",
    "    letters = [ch for ch in tok if ch.isalpha()]\n",
    "    # Require at least 2 letters and all must be uppercase\n",
    "    return (len(letters) >= 2) and all(ch.isupper() for ch in letters)\n",
    "\n",
    "\n",
    "def compute_features_for_text(text: str):\n",
    "    \"\"\"\n",
    "    Extract hand-crafted linguistic features from text for sentiment analysis.\n",
    "    \n",
    "    This function computes 15 normalized features that capture:\n",
    "    - Text length and complexity\n",
    "    - Punctuation patterns\n",
    "    - Stylistic markers (caps, elongation)\n",
    "    - Sentiment modifiers (negations, intensifiers)\n",
    "    - Emotional indicators (emoticons)\n",
    "    \"\"\"\n",
    "    # Tokenize text using pre-defined tokenizer pattern\n",
    "    toks = _tok.findall(text)\n",
    "    toks_lower = [t.lower() for t in toks]\n",
    "    \n",
    "    # Filter to word tokens (containing at least one letter)\n",
    "    word_tokens = [t for t in toks if any(ch.isalpha() for ch in t)]\n",
    "    T = len(word_tokens)  # Total number of word tokens\n",
    "    L = len(text)         # Total character length\n",
    "    \n",
    "    # PUNCTUATION FEATURES: Capture emotional expression through punctuation\n",
    "    count_exclaim = text.count(\"!\")        # Excitement/emphasis\n",
    "    count_question = text.count(\"?\")       # Questions/uncertainty\n",
    "    count_period = text.count(\".\")         # Sentence boundaries\n",
    "    count_comma = text.count(\",\")          # Clause complexity\n",
    "    # Total non-alphanumeric, non-space characters\n",
    "    count_punct_total = sum(1 for ch in text if (not ch.isalnum()) and (not ch.isspace()))\n",
    "    \n",
    "    # STYLISTIC FEATURES: Capture writing style indicators\n",
    "    count_upper_tokens = sum(1 for t in word_tokens if is_all_caps_token(t))\n",
    "    ratio_upper_tokens = (count_upper_tokens / T) if T > 0 else 0.0\n",
    "    \n",
    "    # Elongated words indicate emphasis, any character repeated 3+ times\n",
    "    count_elongated = sum(1 for t in toks_lower \n",
    "                         if any(ch.isalpha() for ch in t) and re.search(r\"(.)\\1{2,}\", t))\n",
    "    \n",
    "    # SENTIMENT MODIFIER FEATURES\n",
    "    count_negations = sum(1 for t in toks_lower if t in NEGATIONS) + text.lower().count(\"n't\")\n",
    "    \n",
    "    # Intensifiers amplify sentiment strength\n",
    "    count_intensifiers = sum(1 for t in toks_lower if t in INTENSIFIERS)\n",
    "    \n",
    "    # EMOTICON FEATURES: Direct sentiment indicators\n",
    "    tl = text.lower()\n",
    "    count_pos_emotes = sum(tl.count(e.strip()) for e in POS_EMOTES)\n",
    "    count_neg_emotes = sum(tl.count(e.strip()) for e in NEG_EMOTES)\n",
    "    \n",
    "    # LENGTH FEATURES: Basic text statistics\n",
    "    len_tokens = float(T)\n",
    "    len_chars = float(L)\n",
    "    avg_tok_len = (len_chars / len_tokens) if T > 0 else 0.0\n",
    "    \n",
    "    # NORMALIZATION: Scale features to comparable ranges\n",
    "    T_norm = max(T, 1.0)\n",
    "    \n",
    "    # Normalize each feature to prevent any single feature from dominating\n",
    "    feats = {\n",
    "        # Length features: scale relative to text size\n",
    "        \"len_tokens_norm\": len_tokens / max(T_norm, 20.0),\n",
    "        \"len_chars_norm\": len_chars / (4.0 * T_norm),  # Assume ~4 chars per token\n",
    "        \"avg_tok_len_norm\": avg_tok_len / 10.0,\n",
    "        \n",
    "        # Punctuation: normalize by token count\n",
    "        \"count_exclaim_norm\": count_exclaim / T_norm,\n",
    "        \"count_question_norm\": count_question / T_norm,\n",
    "        \"count_period_norm\": count_period / T_norm,\n",
    "        \"count_comma_norm\": count_comma / T_norm,\n",
    "        \"count_punct_total_norm\": count_punct_total / max(L, 1.0),\n",
    "        \n",
    "        # Uppercase: scale by half token count (typically rare)\n",
    "        \"count_upper_tokens_norm\": count_upper_tokens / max(T_norm/2.0, 1.0),\n",
    "        \"ratio_upper_tokens_norm\": ratio_upper_tokens,  # Already a ratio\n",
    "        \n",
    "        # Style modifiers: normalize by token count\n",
    "        \"count_elongated_norm\": count_elongated / T_norm,\n",
    "        \"count_negations_norm\": count_negations / T_norm,\n",
    "        \"count_intensifiers_norm\": count_intensifiers / T_norm,\n",
    "        \n",
    "        # Emoticons: raw counts (typically 0-2)\n",
    "        \"count_pos_emotes_norm\": float(count_pos_emotes),\n",
    "        \"count_neg_emotes_norm\": float(count_neg_emotes),\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "\n",
    "print(\"Building feature validation dataset...\")\n",
    "feat_rows = []\n",
    "\n",
    "# Extract features for each training example\n",
    "for idx, row in train_df.iterrows():\n",
    "    feats = compute_features_for_text(row['sentence'])\n",
    "    feats[\"label\"] = row['label']  # Attach ground truth label\n",
    "    feat_rows.append(feats)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_features = pd.DataFrame(feat_rows)\n",
    "\n",
    "# Get list of normalized feature columns (exclude label)\n",
    "feature_cols = [c for c in df_features.columns if c.endswith(\"_norm\")]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(df_features)}\")\n",
    "\n",
    "\n",
    "# Prepare feature matrix and labels\n",
    "X_chi = df_features[feature_cols].values\n",
    "y_chi = df_features[\"label\"].values\n",
    "\n",
    "# Chi-square test measures dependency between each feature and the label\n",
    "# Higher chi2 score = stronger association with sentiment\n",
    "chi_vals, p_vals = chi2(X_chi, y_chi)\n",
    "\n",
    "# Create ranking table\n",
    "chi_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"chi2_score\": chi_vals,      # Higher = more informative\n",
    "    \"p_value\": p_vals            # Lower = more statistically significant\n",
    "}).sort_values(\"chi2_score\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Ranking (Chi-square):\")\n",
    "print(chi_df.to_string(index=False))\n",
    "\n",
    "# Extract features from validation set\n",
    "val_feat_rows = []\n",
    "for idx, row in val_df.iterrows():\n",
    "    feats = compute_features_for_text(row['sentence'])\n",
    "    feats[\"label\"] = row['label']\n",
    "    val_feat_rows.append(feats)\n",
    "\n",
    "df_val = pd.DataFrame(val_feat_rows)\n",
    "\n",
    "# Prepare train/val splits\n",
    "X_train_lr = df_features[feature_cols].values\n",
    "y_train_lr = df_features[\"label\"].values\n",
    "X_val_lr = df_val[feature_cols].values\n",
    "y_val_lr = df_val[\"label\"].values\n",
    "\n",
    "# Standardize features (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_lr)\n",
    "X_val_scaled = scaler.transform(X_val_lr)\n",
    "\n",
    "# Train logistic regression classifier\n",
    "# This establishes a baseline: how well can hand-crafted features perform?\n",
    "lr_model = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "lr_model.fit(X_train_scaled, y_train_lr)\n",
    "\n",
    "# Evaluate on validation set\n",
    "lr_pred = lr_model.predict(X_val_scaled)\n",
    "lr_acc = accuracy_score(y_val_lr, lr_pred)\n",
    "\n",
    "print(f\"\\nLR Baseline validation accuracy: {lr_acc:.4f}\")\n",
    "print(f\"Conclusion: Hand-crafted features alone achieve {lr_acc*100:.1f}% classification ability\")\n",
    "\n",
    "# Group features by category for systematic analysis\n",
    "feature_groups = {\n",
    "    \"length\": [\"len_tokens_norm\",\"len_chars_norm\",\"avg_tok_len_norm\"],\n",
    "    \"punctuation\": [\"count_exclaim_norm\",\"count_question_norm\",\"count_period_norm\",\n",
    "                   \"count_comma_norm\",\"count_punct_total_norm\"],\n",
    "    \"uppercase\": [\"count_upper_tokens_norm\",\"ratio_upper_tokens_norm\"],\n",
    "    \"elongated\": [\"count_elongated_norm\"],\n",
    "    \"negations\": [\"count_negations_norm\"],\n",
    "    \"intensifiers\": [\"count_intensifiers_norm\"],\n",
    "    \"emoticons\": [\"count_pos_emotes_norm\",\"count_neg_emotes_norm\"],\n",
    "}\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "# For each feature group, remove it and measure performance drop\n",
    "# This reveals which features contribute most to classification\n",
    "for group_name, group_feats in feature_groups.items():\n",
    "    # Remove current group's features\n",
    "    remaining_feats = [f for f in feature_cols if f not in group_feats]\n",
    "    \n",
    "    X_train_abl = df_features[remaining_feats].values\n",
    "    X_val_abl = df_val[remaining_feats].values\n",
    "    \n",
    "    # Standardize remaining features\n",
    "    scaler_abl = StandardScaler()\n",
    "    X_train_abl_scaled = scaler_abl.fit_transform(X_train_abl)\n",
    "    X_val_abl_scaled = scaler_abl.transform(X_val_abl)\n",
    "    \n",
    "    # Train and evaluate with reduced feature set\n",
    "    lr_abl = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "    lr_abl.fit(X_train_abl_scaled, y_train_lr)\n",
    "    pred_abl = lr_abl.predict(X_val_abl_scaled)\n",
    "    acc_abl = accuracy_score(y_val_lr, pred_abl)\n",
    "    \n",
    "    # Record performance drop when this group is removed\n",
    "    ablation_results.append({\n",
    "        \"removed_group\": group_name,\n",
    "        \"val_acc\": acc_abl,\n",
    "        \"acc_drop\": lr_acc - acc_abl  # Positive = removing hurts performance\n",
    "    })\n",
    "\n",
    "# Sort by accuracy drop to identify most important feature groups\n",
    "ablation_df = pd.DataFrame(ablation_results).sort_values(\"acc_drop\", ascending=False)\n",
    "print(\"\\nAblation Study Results (performance after removing feature groups):\")\n",
    "print(ablation_df.to_string(index=False))\n",
    "\n",
    "# Select features with chi-square score above threshold\n",
    "# This filters out weakly predictive features\n",
    "IMPORTANCE_THRESHOLD = 10.0\n",
    "selected_features = chi_df[chi_df[\"chi2_score\"] > IMPORTANCE_THRESHOLD][\"feature\"].tolist()\n",
    "\n",
    "print(f\"Based on chi-square > {IMPORTANCE_THRESHOLD}, selected {len(selected_features)} features:\")\n",
    "for feat in selected_features:\n",
    "    chi_val = chi_df[chi_df[\"feature\"] == feat][\"chi2_score\"].values[0]\n",
    "    print(f\"   - {feat:<30} (chi2={chi_val:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d4f34",
   "metadata": {},
   "source": [
    "## DataLoader Setup\n",
    "\n",
    "Create efficient data pipelines that feed batches of samples to the model during training.\n",
    "\n",
    "### Dataset Class\n",
    "\n",
    "Wrap preprocessed data in PyTorch's Dataset interface. PyTorch's DataLoader requires this structure for efficient batching and data loading\n",
    "\n",
    "This pipeline ensures our text data is properly prepared for the LSTM model while maintaining reproducibility and preventing data leakage.\n",
    "\n",
    "\n",
    "### Compute Numeric Feature Function\n",
    "\n",
    "The function processes batches of tokenized text in real-time, converting token IDs back to their string representations to analyze character-level patterns (punctuation, capitalization, emoticons) that aren't captured by word embeddings alone. These features are then normalized to comparable scales and concatenated with the neural network's learned representations, creating a richer input that combines learned semantic knowledge with explicit linguistic rules.\n",
    "\n",
    "In the neural network forward pass, this function is called to compute features for each batch.\n",
    "\n",
    "### Collate Function \n",
    "\n",
    "Sentences have variable lengths (e.g., 5 words vs 20 words), but neural networks require fixed-size tensors. To address this, pad shorter sequences with special `<pad>` tokens to match the longest sequence in each batch\n",
    "\n",
    "### Training DataLoader\n",
    "- `shuffle=True`: Randomize sample order each epoch to prevent overfitting to data order, which is critical for good generalization\n",
    "\n",
    "- `batch_size=128`: Balance between speed and memory, 128 is a common choice for medium datasets\n",
    "\n",
    "### Validation/Test DataLoaders\n",
    "- `shuffle=False`: Keep fixed order to reproducible evaluation metrics\n",
    "\n",
    "- `pin_memory=True`: Speed up CPU→GPU transfer. Only beneficial when using CUDA\n",
    "\n",
    "This setup ensures our data flows efficiently from disk → CPU → GPU during training while maintaining reproducibility for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom PyTorch Dataset Class for accessing individual samples during training\n",
    "class SST2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for SST-2 sentiment classification.\n",
    "    Converts text sentences into numerical representations (token IDs) and pairs them with their corresponding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, stoi):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by preprocessing all samples.\n",
    "        \"\"\"\n",
    "        # Pre-process all data and store in memory\n",
    "        self.data = []\n",
    "        # Iterate through each row in the dataframe\n",
    "        for _, row in dataframe.iterrows():\n",
    "            sentence = row['sentence']\n",
    "            label = int(row['label'])\n",
    "            # Convert sentence to list of integer IDs\n",
    "            ids = [stoi.get(w, stoi[UNK]) for w in tokenize(sentence)]\n",
    "            # Store as dictionary for easy access\n",
    "            self.data.append({'input_ids': ids, 'label': label})\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample at the given index.\n",
    "        This is called by DataLoader to fetch individual samples.\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate dataset objects for each split\n",
    "# These objects will be used by DataLoader during training\n",
    "train_ds = SST2Dataset(train_df, stoi)\n",
    "val_ds = SST2Dataset(val_df, stoi)\n",
    "test_ds = SST2Dataset(test_df, stoi)\n",
    "# Display dataset sizes for verification\n",
    "print(f\"Dataset sizes - Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "\n",
    "# Extract hand-crafted linguistic features from tokenized text batches.\n",
    "def compute_numeric_features(x_ids, pad_idx, selected_feat_names):\n",
    "    \"\"\"\n",
    "    This function converts token IDs back to text and computes numerical features\n",
    "    that capture sentiment-related linguistic patterns. These features complement\n",
    "    neural network representations by explicitly encoding domain knowledge about\n",
    "    sentiment expression.\n",
    "    \"\"\"\n",
    "    # Preserve the device (CPU/GPU) of input tensor to ensure output stays on same device\n",
    "    device = x_ids.device\n",
    "    \n",
    "    # Get batch size (B) and sequence length (T)\n",
    "    B, T = x_ids.size()\n",
    "    \n",
    "    # List to accumulate feature vectors for each sample in the batch\n",
    "    feats_list = []\n",
    "    \n",
    "    # Process each text sample in the batch independently\n",
    "    for i in range(B):\n",
    "        # Extract token IDs for current sample and convert to Python list\n",
    "        ids = x_ids[i].tolist()\n",
    "        \n",
    "        # Remove padding tokens - only analyze actual content\n",
    "        ids = [t for t in ids if t != pad_idx]\n",
    "        \n",
    "        # Map token IDs back to string tokens using vocabulary\n",
    "        toks = [TEXT.vocab.itos[t] for t in ids]\n",
    "        \n",
    "        # Reconstruct the original text by joining tokens with spaces\n",
    "        text = \" \".join(toks)\n",
    "        \n",
    "        # Compute basic text statistics\n",
    "        len_tokens = float(len(toks))          # Number of word tokens\n",
    "        len_chars = float(len(text))           # Total character count\n",
    "        avg_tok_len = (len_chars / len_tokens) if len_tokens > 0 else 0.0  # Average word length\n",
    "        \n",
    "        # Count punctuation marks (emotional expression indicators)\n",
    "        count_exclaim = float(text.count(\"!\"))     # Exclamation marks (emphasis/excitement)\n",
    "        count_question = float(text.count(\"?\"))    # Question marks (uncertainty/inquiry)\n",
    "        count_period = float(text.count(\".\"))      # Periods (sentence structure)\n",
    "        count_comma = float(text.count(\",\"))       # Commas (clause complexity)\n",
    "        \n",
    "        # Count all non-alphanumeric, non-space characters (overall punctuation density)\n",
    "        count_punct_total = float(sum(1 for ch in text if (not ch.isalnum()) and (not ch.isspace())))\n",
    "        \n",
    "        # Analyze capitalization patterns (emphasis/shouting)\n",
    "        count_upper_tokens = float(sum(1 for tok in toks if is_all_caps_token(tok)))\n",
    "        \n",
    "        # Ratio of uppercase tokens to total tokens\n",
    "        ratio_upper_tokens = (count_upper_tokens / len_tokens) if len_tokens > 0 else 0.0\n",
    "        \n",
    "        # Detect elongated words (informal emphasis)\n",
    "        count_elongated = float(sum(1 for tok in toks if re.search(r\"(.)\\1{2,}\", tok or \"\")))\n",
    "        \n",
    "        # Count sentiment modifiers\n",
    "        toks_lower = [t.lower() for t in toks]\n",
    "        \n",
    "        # Count negation words (sentiment polarity reversers)\n",
    "        count_negations = float(sum(1 for t in toks_lower if t in NEGATIONS))\n",
    "        \n",
    "        # Count intensifier words (sentiment amplifiers)\n",
    "        count_intensifiers = float(sum(1 for t in toks_lower if t in INTENSIFIERS))\n",
    "        \n",
    "        # Count emoticons (direct sentiment signals)\n",
    "        tl = text.lower()  # Lowercase for case-insensitive emoticon matching\n",
    "        \n",
    "        # Sum occurrences of positive emoticons\n",
    "        count_pos_emotes = float(sum(tl.count(e.strip()) for e in POS_EMOTES))\n",
    "        \n",
    "        # Sum occurrences of negative emoticons\n",
    "        count_neg_emotes = float(sum(tl.count(e.strip()) for e in NEG_EMOTES))\n",
    "        \n",
    "        # Store raw feature values\n",
    "        all_feats = {\n",
    "            \"len_tokens\": len_tokens,\n",
    "            \"len_chars\": len_chars,\n",
    "            \"avg_tok_len\": avg_tok_len,\n",
    "            \"count_exclaim\": count_exclaim,\n",
    "            \"count_question\": count_question,\n",
    "            \"count_period\": count_period,\n",
    "            \"count_comma\": count_comma,\n",
    "            \"count_punct_total\": count_punct_total,\n",
    "            \"count_upper_tokens\": count_upper_tokens,\n",
    "            \"ratio_upper_tokens\": ratio_upper_tokens,\n",
    "            \"count_elongated\": count_elongated,\n",
    "            \"count_negations\": count_negations,\n",
    "            \"count_intensifiers\": count_intensifiers,\n",
    "            \"count_pos_emotes\": count_pos_emotes,\n",
    "            \"count_neg_emotes\": count_neg_emotes,\n",
    "        }\n",
    "        \n",
    "        # Use actual token count for normalization, with minimum of 1.0 to avoid division by zero\n",
    "        T_norm = max(T, 1.0)\n",
    "        \n",
    "        all_feats_norm = {\n",
    "            # Length features: normalize by typical text size\n",
    "            \"len_tokens_norm\": all_feats[\"len_tokens\"] / max(T_norm, 20.0),\n",
    "            \"len_chars_norm\": all_feats[\"len_chars\"] / (4.0 * T_norm),  # Assume ~4 chars/token\n",
    "            \"avg_tok_len_norm\": all_feats[\"avg_tok_len\"] / 10.0,         # Scale to 0-1 range\n",
    "            \n",
    "            # Punctuation features: normalize by token count (frequency per token)\n",
    "            \"count_exclaim_norm\": all_feats[\"count_exclaim\"] / T_norm,\n",
    "            \"count_question_norm\": all_feats[\"count_question\"] / T_norm,\n",
    "            \"count_period_norm\": all_feats[\"count_period\"] / T_norm,\n",
    "            \"count_comma_norm\": all_feats[\"count_comma\"] / T_norm,\n",
    "            \"count_punct_total_norm\": all_feats[\"count_punct_total\"] / max(len(text), 1.0),\n",
    "            \n",
    "            # Uppercase features: scale by half token count (uppercase is typically rare)\n",
    "            \"count_upper_tokens_norm\": all_feats[\"count_upper_tokens\"] / max(T_norm/2.0, 1.0),\n",
    "            \"ratio_upper_tokens_norm\": all_feats[\"ratio_upper_tokens\"],  # Already a ratio (0-1)\n",
    "            \n",
    "            # Style modifier features: normalize by token count\n",
    "            \"count_elongated_norm\": all_feats[\"count_elongated\"] / T_norm,\n",
    "            \"count_negations_norm\": all_feats[\"count_negations\"] / T_norm,\n",
    "            \"count_intensifiers_norm\": all_feats[\"count_intensifiers\"] / T_norm,\n",
    "            \n",
    "            # Emoticon features: use raw counts (typically 0-2, already small scale)\n",
    "            \"count_pos_emotes_norm\": all_feats[\"count_pos_emotes\"],\n",
    "            \"count_neg_emotes_norm\": all_feats[\"count_neg_emotes\"],\n",
    "        }\n",
    "        \n",
    "        # Extract only the selected features\n",
    "        selected_vals = [all_feats_norm[fname] for fname in selected_feat_names]\n",
    "        \n",
    "        # Add this sample's feature vector to the batch list\n",
    "        feats_list.append(selected_vals)\n",
    "    \n",
    "    # Convert to tensor and return\n",
    "    return torch.tensor(feats_list, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "# Combines multiple samples into a single batch\n",
    "# Neural networks require fixed-size inputs for efficient batch processing\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to process a batch of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to collect input sequences and labels\n",
    "    ids_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Extract data from each sample in the batch\n",
    "    for item in batch:\n",
    "        ids = item['input_ids']    # List of token IDs (variable length)\n",
    "        label = item['label']       # Integer label (0 or 1)\n",
    "        ids_list.append(ids)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Find the maximum sequence length in this batch\n",
    "    max_len = max(len(x) for x in ids_list)\n",
    "    \n",
    "    # Create a tensor filled with padding_idx (0)\n",
    "    # All positions are initialized to padding_idx and will be overwritten with actual tokens\n",
    "    x = torch.full((len(ids_list), max_len), padding_idx, dtype=torch.long)\n",
    "    \n",
    "    # Fill in the actual token IDs for each sequence\n",
    "    for i, ids in enumerate(ids_list):\n",
    "        # Copy token IDs to the beginning of each row\n",
    "        x[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    # Convert labels list to tensor\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # Return as SimpleNamespace for convenient attribute access\n",
    "    return types.SimpleNamespace(text=x, label=y)\n",
    "\n",
    "# Create DataLoader Objects\n",
    "# PyTorch utility that handles batching, shuffling, and parallel data loading\n",
    "\n",
    "# Training DataLoader\n",
    "train_iter = DataLoader(\n",
    "    train_ds,              # Dataset object to load from\n",
    "    batch_size=128,        # Number of samples per batch\n",
    "    shuffle=True,          # Randomly shuffle data each epoch\n",
    "    collate_fn=collate_fn, # Our custom function to combine samples into batches\n",
    "    pin_memory=True        # If True, allocates tensors in pinned memory\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "val_iter = DataLoader(\n",
    "    val_ds,                # Validation dataset\n",
    "    batch_size=128,        # Same batch size as training for consistency\n",
    "    shuffle=False,         # DON'T shuffle validation data\n",
    "    collate_fn=collate_fn, # Same collate function\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Test DataLoader\n",
    "test_iter = DataLoader(\n",
    "    test_ds,               # Test dataset\n",
    "    batch_size=128,\n",
    "    shuffle=False,         # DON'T shuffle test data\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Get the first batch from training DataLoader\n",
    "first = next(iter(train_iter))\n",
    "\n",
    "# Display batch information for verification\n",
    "print(f\"\\nBatch info:\")\n",
    "print(f\"Type: {type(first)}\")  \n",
    "print(f\"Text shape: {first.text.shape}\")  \n",
    "print(f\"Label shape: {first.label.shape}\")  \n",
    "print(f\"Sample text (first 10 tokens): {first.text[0][:10]}\")  \n",
    "print(f\"Sample label: {first.label[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b794b5e",
   "metadata": {},
   "source": [
    "## BiLSTM Sentiment Classification Model\n",
    "\n",
    "For sentiment classification, we employ a Bidirectional Long Short-Term Memory (BiLSTM) network as our primary algorithm. This choice is justified by several key advantages:\n",
    "\n",
    "1. Sequential Nature of Language: Text is not just a collection of random words—the order matters tremendously. LSTMs are designed specifically to process sequences where order matters. As the LSTM reads through a sentence word by word, it maintains a \"memory\" of what it has seen before. \n",
    "\n",
    "2. Long-Range Dependency Modeling: Sometimes the key to understanding sentiment is remembering something from much earlier in the text. RNNs  try to do this but fail because they forget information from earlier in the sequence. LSTMs solve this problem using special mechanisms called \"gates\" that act like memory controllers. \n",
    "\n",
    "3. Bidirectional Context Capture: A regular LSTM only reads left-to-right (forward). A Bidirectional LSTM  reading the sentence in both directions simultaneously: one LSTM reads left-to-right (forward), and another reads right-to-left (backward). By combining information from both directions, the model gets the complete picture, leading to much better understanding of sentiment.\n",
    "\n",
    "4. Hierarchical Representation Learning: We use 2 LSTM layers stacked on top of each other, which allows the model to learn patterns at different levels of abstraction. Our first LSTM layer learns basic patterns like grammar and simple word combinations. The second LSTM layer then takes these basic patterns and learns more complex, abstract patterns. \n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### 1. Classification Accuracy\n",
    "\n",
    "We mainly use classification accuracy as our primary performance measure. \n",
    "\n",
    "Classification accuracy is the most appropriate metric for our sentiment analysis task. First, our dataset exhibits balanced class distribution—we have roughly equal numbers of positive and negative reviews, meaning accuracy won't be misleadingly inflated by predicting the majority class.\n",
    "\n",
    "Second, for binary sentiment classification, both classes (positive/negative) are equally important. We don't have an asymmetric cost structure where false positives are more costly than false negatives, so accuracy's equal weighting of all errors is appropriate.\n",
    "\n",
    "#### 2. Cross-Entropy Loss\n",
    "\n",
    "We also monitor cross-entropy loss during training and validation.\n",
    "\n",
    "Cross-entropy loss measures the divergence between predicted probability distributions and true labels, providing a more nuanced signal than accuracy during training. Cross-entropy rewards confident correct predictions and penalizes confident wrong predictions more heavily. \n",
    "\n",
    "Cross-entropy is the natural choice for neural network classification because it's the negative log-likelihood of the correct class, making it theoretically grounded in maximum likelihood estimation. It's also convex with respect to the final layer's logits, ensuring stable gradient descent optimization. \n",
    "\n",
    "### Overfitting and Underfitting Prevention\n",
    "\n",
    "#### 1. Overfitting Prevention\n",
    "\n",
    "1. Frozen Pretrained Embeddings: GloVe embeddings are pretrained on billions of words from Wikipedia and news corpora. These vectors already encode rich semantic relationships. By freezing these weights, we prevent the model from overfitting embeddings to our small training set (6,568 samples). \n",
    "\n",
    "\n",
    "2. Dropout Regularization: Dropout randomly sets activations to zero during training with probability `p`, forcing the network to learn redundant representations. If the model relies on a single neuron to detect \"not,\" and that neuron is dropped out 50% of the time, the model must learn alternative pathways to detect negation.\n",
    "\n",
    "3. Learning Rate Scheduling: Learning rate scheduling automatically adjusts the learning rate during training to balance fast learning with precise convergence. Early in training, a high learning rate (0.15) allows the model to quickly explore the parameter space and make rapid progress toward good solutions. However, as training progresses, these large update steps cause the model to overshoot optimal parameter values, bouncing around the best solution rather than settling into it. The scheduler monitors validation loss and reduces the learning rate by 10% (factor=0.9) when performance plateaus for 3 consecutive epochs.\n",
    "\n",
    "4. Early Stopping with Model Checkpointing\n",
    "\n",
    "Training for too many epochs inevitably leads to overfitting—the model continues improving on training data while validation performance degrades. Early stopping prevents this by monitoring validation loss and saving the model checkpoint when it achieves the lowest validation loss. After 50 epochs, even if the final model is overfit, we retain the best-performing model from earlier in training (typically around epochs 15-25).\n",
    "\n",
    "5. Train-Validation Monitoring\n",
    "\n",
    "Explicit monitoring of the train-validation gap provides a human-interpretable diagnostic for model health. A gap of 2-5% is normal and acceptable—it indicates the model has learned patterns specific to training data but still generalizes well. A gap exceeding 10% signals severe overfitting, prompting us to stop training, increase regularization, or reduce model capacity.\n",
    "\n",
    "### 2. Underfitting Prevention\n",
    "\n",
    "1. Sufficient Model Capacity: 2-layer BiLSTM with 384 hidden units provides `384 × 2 directions × 2 layers = 1,536` hidden states across the network, sufficient to learn nuanced linguistic patterns.\n",
    "\n",
    "2. Adequate Training Duration: Training for 50 epochs with early stopping ensures we explore enough of the loss landscape. If validation loss is still decreasing at epoch 50, we'd extend training, but typically loss plateaus around epoch 20-30.\n",
    "\n",
    "3. High Initial Learning Rate: Starting with LR=0.15 ensures rapid initial learning. Too small a learning rate (e.g., 0.001) would cause extremely slow convergence, potentially getting stuck in poor local minima and resulting in underfitting.\n",
    "\n",
    "4. Monitoring Training Loss: If training loss remains high (>0.5) throughout training, this signals underfitting—the model lacks capacity or training time to fit the data. We'd respond by adding layers, or training longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 384  # LSTM hidden state dimension\n",
    "label_size = 2    # Binary classification: positive (1) vs negative (0)\n",
    "num_selected_features = len(selected_features)  # Number of hand-crafted features to use\n",
    "\n",
    "# Display model architecture configuration for reproducibility\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  - Vocabulary size: {vocab_size}\")           # Total unique tokens\n",
    "print(f\"  - Embedding dimension: {embedding_dim}\")     # GloVe vector size (100)\n",
    "print(f\"  - LSTM hidden dimension: {hidden_dim}\")      # Hidden state size\n",
    "print(f\"  - Number of features used: {num_selected_features}\")  # Selected features\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM sentiment classifier with hand-crafted feature fusion.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Embedding Layer: Maps token IDs to GloVe vectors (frozen)\n",
    "        2. BiLSTM Layers: Captures bidirectional sequential context (2 layers, dropout 0.5)\n",
    "        3. Feature Network: Projects hand-crafted features to dense representation\n",
    "        4. Fusion Layer: Concatenates LSTM output with feature vector\n",
    "        5. Classification Head: Final fully-connected layer for sentiment prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, \n",
    "                 padding_idx, num_features, selected_feat_names):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid BiLSTM classifier.\n",
    "        \"\"\"\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer, create embedding lookup table: vocab_size x embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        # Load pretrained GloVe vectors into embedding weights\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        # Don't update during training\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Bidirectuinal LSTM (2 layers, dropout 0.5)\n",
    "        # - Forward LSTM: reads text left-to-right (past → present)\n",
    "        # - Backward LSTM: reads text right-to-left (future → present)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,      # Input size: word embedding dimension (100)\n",
    "            hidden_dim,         # Hidden state size: 384 per direction\n",
    "            num_layers=2,       # Stack 2 LSTM layers for deeper representation\n",
    "            batch_first=True,   # Input shape: (batch, seq_len, features)\n",
    "            dropout=0.5,        # Dropout between LSTM layers (regularization)\n",
    "            bidirectional=True  # Process sequence in both directions\n",
    "        )\n",
    "        \n",
    "        # Hand-crafted feature projection network\n",
    "        self.numeric_proj = nn.Sequential(\n",
    "            nn.Linear(num_features, 32),  # Project features to 32-dim space\n",
    "            nn.ReLU(),                     # Non-linear activation\n",
    "            nn.Dropout(0.2)                # Light dropout for regularization\n",
    "        )\n",
    "        \n",
    "        # Fusion + Classification head\n",
    "        # Fuse LSTM representation (768-dim) with feature vector (32-dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 + 32, label_size)\n",
    "        \n",
    "        # Dropout for embedding layer (applied after embedding lookup)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Store padding index and feature names for forward pass\n",
    "        self.pad_idx = padding_idx\n",
    "        self.selected_feat_names = selected_feat_names\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Convert token IDs to sentiment predictions.\n",
    "        \"\"\"\n",
    "        # Embed tokens (with dropout regularization)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Process sequence through bidirectional LSTM\n",
    "        # We only need the final hidden states from both directions\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Extract final layer's hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        # Apply dropout to LSTM output for regularization\n",
    "        sent_vec = self.dropout(hidden)\n",
    "        \n",
    "        # Compute linguistic features from token IDs\n",
    "        numeric_feats = compute_numeric_features(x, self.pad_idx, self.selected_feat_names)\n",
    "        \n",
    "        # Project features through small neural network\n",
    "        numeric_vec = self.numeric_proj(numeric_feats)\n",
    "        \n",
    "        # Concatenate LSTM output (768-dim) with feature projection (32-dim)\n",
    "        fused = torch.cat([sent_vec, numeric_vec], dim=1)\n",
    "    \n",
    "        # Map fused representation to class logits\n",
    "        return self.fc(fused)\n",
    "\n",
    "\n",
    "# Create the hybrid BiLSTM classifier with all specified parameters\n",
    "model = BiLSTMClassifier(\n",
    "    vocab_size,              # Total vocabulary size\n",
    "    embedding_dim,           # GloVe embedding dimension (100)\n",
    "    hidden_dim,              # LSTM hidden size (384)\n",
    "    label_size,              # Binary classification (2)\n",
    "    padding_idx,             # Padding token index\n",
    "    num_selected_features,   # Number of hand-crafted features\n",
    "    selected_features        # List of feature names to extract\n",
    ")\n",
    "\n",
    "# Move model to GPU if available (enables parallel computation)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Loss function: Cross-entropy for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Stochastic Gradient Descent with momentum\n",
    "# Only optimize parameters that require gradients (excludes frozen embeddings)\n",
    "optimizer = optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),  # Only trainable params\n",
    "    lr=0.15,        # Learning rate (relatively high for SGD)\n",
    "    momentum=0.9    # Momentum helps escape local minima and smooths updates\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: Reduce LR when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',          # Monitor validation loss (minimize)\n",
    "    factor=0.9,     # Multiply LR by 0.9 when plateau detected\n",
    "    patience=3      # Wait 3 epochs before reducing LR\n",
    ")\n",
    "\n",
    "# Display overfitting prevention strategies for transparency\n",
    "print(\"Overfitting prevention strategies:\")\n",
    "print(\"  1. Freeze pretrained embeddings (reduce parameters)\")\n",
    "print(\"  2. Dropout (0.5 LSTM, 0.2 feature layer)\")\n",
    "print(\"  3. Learning Rate Scheduler (reduce LR when validation stalls)\")\n",
    "print(\"  4. Early Stopping (save best validation model)\")\n",
    "print(\"  5. Monitor train/validation gap\")\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0  # Accumulator for batch losses\n",
    "    \n",
    "    # Iterate through all training batches\n",
    "    for batch in iterator:\n",
    "        # Zero out gradients from previous batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch data to GPU\n",
    "        x = batch.text.to(device, non_blocking=True)   # Input token IDs\n",
    "        y = batch.label.to(device, non_blocking=True)  # Ground truth labels\n",
    "        \n",
    "        # Compute predictions\n",
    "        pred = model(x) \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(pred, y)  # Cross-entropy loss (scalar)\n",
    "        \n",
    "        # Compute gradients via backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using gradients\n",
    "        optimizer.step()  # weights = weights - lr * gradients\n",
    "        \n",
    "        # Accumulate loss for epoch average\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Return average loss across all batches\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation/test set.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Disable gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Move data to GPU\n",
    "            x = batch.text.to(device, non_blocking=True)\n",
    "            y = batch.label.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass only (no backward pass needed)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def accuracy(model, iterator):\n",
    "    \"\"\"\n",
    "    Compute classification accuracy on a dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0  # Count of correct predictions\n",
    "    total = 0    # Total number of samples\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            x = batch.text.to(device, non_blocking=True)\n",
    "            y = batch.label.to(device, non_blocking=True)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            pred = model(x)\n",
    "            \n",
    "            # Convert logits to class predictions\n",
    "            _, predicted = torch.max(pred, 1)  # Get class with highest score\n",
    "            \n",
    "            # Count total samples and correct predictions\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # Return accuracy as percentage (0.0 to 1.0)\n",
    "    return correct / total\n",
    "\n",
    "N_EPOCHS = 50  # Maximum number of training epochs\n",
    "best_valid_loss = float('inf')  # Track best validation loss for early stopping\n",
    "\n",
    "# Lists to store metrics for plotting learning curves\n",
    "train_losses, val_losses = [], []  # Loss trajectories\n",
    "train_accs, val_accs = [], []      # Accuracy trajectories\n",
    "\n",
    "print(f\"\\nStarting training for {N_EPOCHS} epochs...\")\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Train for one epoch\n",
    "    train_loss = train(model, train_iter, optimizer, criterion)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    valid_loss = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    # Comnpute accuracies\n",
    "    train_acc = accuracy(model, train_iter)\n",
    "    valid_acc = accuracy(model, val_iter)\n",
    "    \n",
    "    # Record metrics for analysis\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(valid_acc)\n",
    "    \n",
    "    # Reduce learning rate if validation loss hasn't improved\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    # Keep track of model with lowest validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model = copy.deepcopy(model)  # Deep copy to preserve weights\n",
    "    \n",
    "    # Periodic logging every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch: {epoch+1:02} | LR: {scheduler.get_last_lr()[-1]:.4f}')\n",
    "        print(f'  Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}')\n",
    "        print(f'  Val Loss: {valid_loss:.3f} | Val Acc: {valid_acc:.3f}')\n",
    "        \n",
    "        # Monitor overfitting: Large gap between train/val indicates overfitting\n",
    "        # If train_acc >> val_acc, model is memorizing rather than generalizing\n",
    "        print(f'  Overfitting check: Train/Val Acc gap = {abs(train_acc - valid_acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb48e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy(best_model, test_iter)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"  Logistic Regression Baseline:  {lr_acc:.4f}\")\n",
    "print(f\"  BiLSTM + Feature Fusion (val): {max(val_accs):.4f}\")\n",
    "print(f\"  BiLSTM + Feature Fusion (test):{test_acc:.4f}\")\n",
    "print(f\"  Performance improvement:        +{max(val_accs) - lr_acc:.4f}\")\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"  1. Feature Engineering: Selected {num_selected_features} from 15 candidate features\")\n",
    "print(f\"  2. Feature Validation: LR baseline {lr_acc:.4f}\")\n",
    "print(f\"  3. Deep Model: BiLSTM with feature fusion achieved {test_acc:.4f}\")\n",
    "print(f\"  4. Explainability: Chi-square test + Ablation study validated feature effectiveness\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
